---

author: Federico Bruzzone
title: Hyperparameter and Consistency - Machine Learning pt.4 
date: 2023-06-08
draft: false 
tags: [computer-science, machine-learning, statistics, math, linear-algebra]
categories: [computer-science, machine-learning]

summary: "Statistical Methods for Machine Learning - Questions & Answers pt.4. Hyperparameter Tuning, Risk Estimates, Consistency and nonparametric algorithms."
katex: true
mathjax: true
math: true

;menu:
;  main:
;    parent: "design-patters"
;    weight: 1

---

## Hyperparameter Tuning and Risk Estimates

### Write the formula for the K-fold cross validation estimate. Explain the main quantities occurring in the formula.

The K-fold cross validation estimate of $\mathcal{E}[\ell_{D}(A)]$ on $S$, denoted by $\ell_{S}^{CV}(A)$, is then computed as follows: we run $A$ on each training part $S_{-i}$ of the folds $i = 1, \dots, K$ and obtain the predictors $h_i = A(S_{-i}) \dots, h_K = A(S_{-K})$. We then compute the (rescaled) error on the testing part of each fold,
$$
\ell_{S_{i}}(h_i) = \frac{K}{m} \sum_{(\boldsymbol{x},y) \in S_{i}} \ell(h_i(\boldsymbol{x}), y)
$$
Finally, we compute the CV estimate by averaging these errors:
$$
\ell_{S}^{CV}(A) = \frac{1}{K} \sum_{i=1}^{K} \ell_{S_{i}}(h_i)
$$

---

### Write the pseudo-code for computing the nested cross validation estimate.

**Input**: Dataset $S$

Split $S$ into folds $S_1, \dots, S_K$

**for** $i = 1, \dots, K$ **do**

1. Compute training part of $i$-th fold: $S_{-i} \equiv S\ \backslash\  S_i$

2. Run CV on $S_{-i}$ for each $\theta \in \Theta_0$ and find $\theta_i = \underset{\theta \in \Theta_0}{\textmd{argmin}}\ell_{S_{-i}}^{CV}(A_{\theta})$

3. Re-train $A_{\theta_i}$ on $S_{-i}: h_i = A_{\theta_i}(S_{-i})$            

4. Compute error of $i$-th fold: $\epsilon_i = \ell_{S_i}(h_i)$

**Output**: $(\epsilon_1 + \dots + \epsilon_K)$

## Consistency and Nonparametric Algorithms

### Write the mathematical definition of consistency for an algorithm $A$.

A learning algorithm $A$ is consistent with respect to a loss function $\ell$ if for any data distribution $D$ it holds that
$$
\lim_{m \to \infty} \mathbb{E} \left[ \ell_{\mathcal{D}}(A(S)) \right] = \ell_{\mathcal{D}}(f^*)
$$
where the expectation is with respect to the random draw of the training set $S_m$ of size $m$ from the distribution $\mathcal{D}$, and $\ell_{\mathcal{D}}(f)$ is the Bayes risk risk for $(\mathcal{D}, f)$.

### Write the statement of the no-free-lunch theorem.

For any sequence $a_1, a_2, \dots$ of positive numbers converging to zero and such that $\frac{1}{16} \geq a_1 \geq a_2 \geq \dots$ and for any consistent learning algorithm $A$ for binary classification with zero-one loss, there exists a data distribution $\mathcal{D}$ such that $\ell_{\mathcal{D}}f^* = 0$ and $\mathbb{E}[\ell_{\mathcal{D}}(A(S_m))] \geq a_m$ for all $m \geq 1$.

### Write the mathematical definition of nonparametric learning algorithm. Define the main quantities occurring in the formula.

Given a learning algorithm $A$, let $\mathcal{H}\_m$ be the set of predictors generated by $A$ on training sets of size $m: h \in \mathcal{H}\_m$ if and only if there exists a training set $S_m$ of size $m$ such that $A(S_m) = h$. We say that $A$ is a nonparametric learning algorithm if $A$'s approximation error vanishes as $m$ grows to infinity. Formally,
$$
\lim\_{m \to \infty} \min\_{h \in \mathcal{H}\_m} \ell\_{\mathcal{D}}(h^*) = \ell_{\mathcal{D}}(f^\*) 
$$


### Name one nonparametric learning algorithm and one parametric learning algorithm.

Nonparametric: k-nearest neighbors and greedy decision tree classifiers
Parametric: linear regression and logistic regression

### Write the mathematical conditions on $k$ ensuring consistency for the $k$-NN algorithm.

We let $k$ be chosen as a function $k_m$ of the training set size, then the algorithm becomes consistent provided $k_m \rightarrow \infty$ and $k_m = o(m)$.

### Write the formula for the Lipschitz condition in a binary classification problem. Define the main quantities occurring in the formula.

In some cases, we may define consistency with respect to a restricted class of distributions $\mathcal{D}$. For example, in binary classification we may restrict to all distribution $\mathcal{D}$ such that $\eta(\boldsymbol(x) = \mathbb{P}(Y = 1 | \boldsymbol{X} = \boldsymbol{x})$ is Lipschitz function on $\mathcal{X}$. Formally, there exists $0 < c < \infty$ such that

\begin{equation} 
\begin{aligned}
\eta(\boldsymbol{x}) - \eta(\boldsymbol{x}')| \leq c \|\boldsymbol{x} - \boldsymbol{x}'\| && \text{for all}\ \boldsymbol{x}, \boldsymbol{x}' \in \mathcal{X} 
\end{aligned}
\end{equation}




